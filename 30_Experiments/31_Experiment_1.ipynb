{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da42dc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV, RandomizedSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning) # Suppress some SHAP/XGBoost warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "pd.options.mode.chained_assignment = None # default='warn', suppress SettingWithCopyWarning for imputation steps\n",
    "\n",
    "# --- Configuration ---\n",
    "TARGET_VARIABLE = 'PCT_PRICE_CHANGE_DETRENDED (%)'\n",
    "RAW_TARGET_VARIABLE = 'PCT_PRICE_CHANGE (%)'\n",
    "YEAR_COLUMN = 'YEAR'\n",
    "ZIP_COLUMN = 'ZIP_CODE'\n",
    "TRAIN_YEAR_CUTOFF = 2020\n",
    "FIRE_EXPOSED_COL = 'FIRE_EXPOSED' # Define the fire exposure column name\n",
    "\n",
    "# --- XGBoost Tuning Parameters ---\n",
    "# Define Parameter Grid for XGBoost\n",
    "XGB_PARAM_GRID = {\n",
    "    'n_estimators': [100, 200, 300],         # Number of trees\n",
    "    'learning_rate': [0.05, 0.1, 0.2],       # Step size shrinkage\n",
    "    'max_depth': [3, 5, 7],                  # Max tree depth\n",
    "    'subsample': [0.7, 0.8, 0.9],            # Fraction of samples per tree\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9],     # Fraction of features per tree\n",
    "    'gamma': [0, 0.1],                       # Minimum loss reduction for split\n",
    "}\n",
    "N_CV_SPLITS = 5 # Number of folds for TimeSeriesSplit\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    data = pd.read_csv(\"../10_Data_Clean/final_data.csv\", dtype={'GEO_UNIQUE_ID': str})\n",
    "    print(\"Data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: xgboost_data.csv not found. Please ensure the file is in the correct directory.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f47b93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 1: Detrending Target Variable ---\n",
      "Target variable 'PCT_PRICE_CHANGE_DETRENDED (%)' created.\n",
      "Target variable mean: 0.0000, std: 10.5283\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Detrending Target Variable ---\n",
    "print(\"\\n--- Step 1: Detrending Target Variable ---\")\n",
    "if YEAR_COLUMN not in data.columns or RAW_TARGET_VARIABLE not in data.columns:\n",
    "     raise ValueError(f\"Required columns '{YEAR_COLUMN}' or '{RAW_TARGET_VARIABLE}' not found in data.\")\n",
    "data[YEAR_COLUMN] = pd.to_numeric(data[YEAR_COLUMN], errors='coerce')\n",
    "data = data.dropna(subset=[YEAR_COLUMN, RAW_TARGET_VARIABLE]).copy()\n",
    "\n",
    "if not data.empty:\n",
    "    trend_model = LinearRegression()\n",
    "    trend_model.fit(data[[YEAR_COLUMN]], data[RAW_TARGET_VARIABLE])\n",
    "    predicted_trend = trend_model.predict(data[[YEAR_COLUMN]])\n",
    "    data[TARGET_VARIABLE] = data[RAW_TARGET_VARIABLE] - predicted_trend\n",
    "    print(f\"Target variable '{TARGET_VARIABLE}' created.\")\n",
    "    print(f\"Target variable mean: {data[TARGET_VARIABLE].mean():.4f}, std: {data[TARGET_VARIABLE].std():.4f}\")\n",
    "else:\n",
    "    raise ValueError(\"Data became empty after dropping NaNs in YEAR or RAW_TARGET_VARIABLE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c685ccfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2: Defining Feature Sets & Imputation Rules ---\n",
      "Note: 'FIRE_EXPOSED' not in baseline_features list, will be added temporarily if needed for grouped imputation.\n",
      "\n",
      "Using Baseline Features (15): ['Median_Household_Income', 'Total_Population', 'Avg_Household_Size', 'Gini_Index', 'Employment_Rate', 'Below_Poverty_Rate', 'Rate_College_or_Higher', 'Black_Portion', 'White_Portion', 'American_Indian_and_Alaska_Native_Portion', 'Asian_Portion', 'HOME_PRICE_LAG1', 'PRICE_CHANGE_LAG1', 'PRICE_CHANGE_DIFF', 'ROLLING_1yr_PRICE_CHANGE']\n",
      "Using Wildfire Features Only (7): ['NUM_FIRES', 'AVG_FIRE_DURATION_DAYS', 'ANY_MAJOR_FIRE', 'FIRE_EXPOSED', 'FIRE_LAST_YEAR', 'FIRE_SHOCK', 'YEARS_SINCE_LAST_FIRE']\n",
      "Using Interaction Features (3): ['FIRE_EXPOSED_x_PRICE_CHANGE_LAG1', 'FIRE_EXPOSED_x_Median_Household_Income', 'FIRE_EXPOSED_x_Below_Poverty_Rate']\n",
      "Using All Features for WF model (25): ['ANY_MAJOR_FIRE', 'AVG_FIRE_DURATION_DAYS', 'American_Indian_and_Alaska_Native_Portion', 'Asian_Portion', 'Avg_Household_Size', 'Below_Poverty_Rate', 'Black_Portion', 'Employment_Rate', 'FIRE_EXPOSED', 'FIRE_EXPOSED_x_Below_Poverty_Rate', 'FIRE_EXPOSED_x_Median_Household_Income', 'FIRE_EXPOSED_x_PRICE_CHANGE_LAG1', 'FIRE_LAST_YEAR', 'FIRE_SHOCK', 'Gini_Index', 'HOME_PRICE_LAG1', 'Median_Household_Income', 'NUM_FIRES', 'PRICE_CHANGE_DIFF', 'PRICE_CHANGE_LAG1', 'ROLLING_1yr_PRICE_CHANGE', 'Rate_College_or_Higher', 'Total_Population', 'White_Portion', 'YEARS_SINCE_LAST_FIRE']\n",
      "\n",
      "Imputation Rules:\n",
      " Zero-fill NaNs for (6): ['NUM_FIRES', 'AVG_FIRE_DURATION_DAYS', 'ANY_MAJOR_FIRE', 'FIRE_LAST_YEAR', 'FIRE_SHOCK', 'FIRE_EXPOSED']\n",
      " Grouped median impute for (8): ['ROLLING_1yr_PRICE_CHANGE', 'Employment_Rate', 'YEARS_SINCE_LAST_FIRE', 'HOME_PRICE_LAG1', 'PRICE_CHANGE_DIFF', 'Below_Poverty_Rate', 'PRICE_CHANGE_LAG1', 'Median_Household_Income']\n",
      " Interaction handling for (3): ['FIRE_EXPOSED_x_PRICE_CHANGE_LAG1', 'FIRE_EXPOSED_x_Median_Household_Income', 'FIRE_EXPOSED_x_Below_Poverty_Rate']\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2: Define Feature Sets & Imputation Rules ---\n",
    "print(\"\\n--- Step 2: Defining Feature Sets & Imputation Rules ---\")\n",
    "\n",
    "# Core Baseline Features\n",
    "baseline_features = [\n",
    "    'Median_Household_Income', 'Total_Population', 'Avg_Household_Size',\n",
    "    'Gini_Index', 'Employment_Rate', 'Below_Poverty_Rate',\n",
    "    'Rate_College_or_Higher', 'Black_Portion', 'White_Portion',\n",
    "    'American_Indian_and_Alaska_Native_Portion', 'Asian_Portion',\n",
    "    'HOME_PRICE_LAG1', 'PRICE_CHANGE_LAG1', 'PRICE_CHANGE_DIFF',\n",
    "    'ROLLING_1yr_PRICE_CHANGE', 'ROLLING_3yr_PRICE_CHANGE_STD'\n",
    "]\n",
    "\n",
    "# Specific Wildfire Features (excluding interactions for now)\n",
    "wildfire_features_only = [\n",
    "    'NUM_FIRES', 'TOTAL_ACRES_BURNED_IN_ZIP', 'AVG_FIRE_DURATION_DAYS',\n",
    "    'MAX_PCT_ZIP_BURNED', 'ANY_MAJOR_FIRE', FIRE_EXPOSED_COL, # Ensure FIRE_EXPOSED_COL is included\n",
    "    'PREV_MAX_PCT_ZIP_BURNED', 'FIRE_LAST_YEAR', 'FIRE_SHOCK',\n",
    "    'YEARS_SINCE_LAST_FIRE', 'CUMULATIVE_ACRES_BURNED_LAST_3YRS'\n",
    "]\n",
    "\n",
    "# Interaction Terms\n",
    "interaction_features = [\n",
    "    'FIRE_EXPOSED_x_PRICE_CHANGE_LAG1',\n",
    "    'MAX_PCT_ZIP_BURNED_x_ROLLING_1yr_PRICE_CHANGE',\n",
    "    'FIRE_EXPOSED_x_Median_Household_Income',\n",
    "    'FIRE_EXPOSED_x_Below_Poverty_Rate'\n",
    "]\n",
    "\n",
    "# Filter features to only those present in the data\n",
    "baseline_features = [f for f in baseline_features if f in data.columns]\n",
    "wildfire_features_only = [f for f in wildfire_features_only if f in data.columns]\n",
    "interaction_features = [f for f in interaction_features if f in data.columns]\n",
    "\n",
    "# --- Define Imputation Rules ---\n",
    "# 1. Features where NaN means zero activity (primarily fire counts/extent)\n",
    "zero_fill_fire_metrics = [\n",
    "    'NUM_FIRES', 'TOTAL_ACRES_BURNED_IN_ZIP', 'AVG_FIRE_DURATION_DAYS',\n",
    "    'MAX_PCT_ZIP_BURNED', 'ANY_MAJOR_FIRE',\n",
    "    'PREV_MAX_PCT_ZIP_BURNED', 'FIRE_LAST_YEAR', 'FIRE_SHOCK',\n",
    "    'CUMULATIVE_ACRES_BURNED_LAST_3YRS',\n",
    "    # FIRE_EXPOSED is binary, NaN is usually invalid here, but treat as 0 if present\n",
    "    FIRE_EXPOSED_COL\n",
    "]\n",
    "zero_fill_fire_metrics = [f for f in zero_fill_fire_metrics if f in data.columns and f in wildfire_features_only]\n",
    "\n",
    "# 2. Interaction term definitions (map interaction term to its components)\n",
    "# Format: {'INTERACTION_TERM': ('FIRE_RELATED_VAR', 'OTHER_VAR')}\n",
    "interaction_pairs = {\n",
    "    'FIRE_EXPOSED_x_PRICE_CHANGE_LAG1': (FIRE_EXPOSED_COL, 'PRICE_CHANGE_LAG1'),\n",
    "    'FIRE_EXPOSED_x_Median_Household_Income': (FIRE_EXPOSED_COL, 'Median_Household_Income'),\n",
    "    'MAX_PCT_ZIP_BURNED_x_ROLLING_1yr_PRICE_CHANGE': ('MAX_PCT_ZIP_BURNED', 'ROLLING_1yr_PRICE_CHANGE'),\n",
    "    'FIRE_EXPOSED_x_Below_Poverty_Rate': (FIRE_EXPOSED_COL, 'Below_Poverty_Rate')\n",
    "}\n",
    "# Filter pairs to only those where term and components exist in the data\n",
    "interaction_pairs = {\n",
    "    k: v for k, v in interaction_pairs.items()\n",
    "    if k in interaction_features and v[0] in data.columns and v[1] in data.columns\n",
    "}\n",
    "\n",
    "# 3. Features for Grouped Median Imputation (based on FIRE_EXPOSED status)\n",
    "grouped_median_impute_cols = [\n",
    "    'Median_Household_Income', 'Employment_Rate',\n",
    "    'ROLLING_1yr_PRICE_CHANGE', 'HOME_PRICE_LAG1', 'PRICE_CHANGE_LAG1', 'PRICE_CHANGE_DIFF',\n",
    "    'YEARS_SINCE_LAST_FIRE', # Can be imputed based on group\n",
    "    # Also include components of interactions if they can be NaN and aren't zero-filled\n",
    "    'Below_Poverty_Rate'\n",
    "]\n",
    "grouped_median_impute_cols = [f for f in grouped_median_impute_cols if f in data.columns]\n",
    "grouped_median_impute_cols = list(set(grouped_median_impute_cols) - set(zero_fill_fire_metrics)) # Avoid double handling\n",
    "\n",
    "# --- Combine Feature Sets ---\n",
    "# Baseline model uses only baseline features\n",
    "# Wildfire model uses baseline + wildfire-only + interactions\n",
    "all_features = sorted(list(set(baseline_features + wildfire_features_only + interaction_features)))\n",
    "\n",
    "# Ensure FIRE_EXPOSED_COL is in the feature lists if needed for imputation/modeling\n",
    "if FIRE_EXPOSED_COL not in baseline_features:\n",
    "    print(f\"Note: '{FIRE_EXPOSED_COL}' not in baseline_features list, will be added temporarily if needed for grouped imputation.\")\n",
    "if FIRE_EXPOSED_COL not in all_features and FIRE_EXPOSED_COL in data.columns:\n",
    "    all_features.append(FIRE_EXPOSED_COL)\n",
    "    all_features = sorted(list(set(all_features)))\n",
    "\n",
    "# Verify no target/future leakage\n",
    "leaky_features = ['PRICE', 'NEXT_YEAR_PRICE', 'PRICE_CHANGE', RAW_TARGET_VARIABLE, TARGET_VARIABLE]\n",
    "for f in leaky_features:\n",
    "    if f in all_features:\n",
    "        print(f\"CRITICAL WARNING: Leaky feature '{f}' found in 'all_features'. Removing.\")\n",
    "        all_features.remove(f)\n",
    "    if f in baseline_features:\n",
    "        print(f\"CRITICAL WARNING: Leaky feature '{f}' found in 'baseline_features'. Removing.\")\n",
    "        baseline_features.remove(f)\n",
    "\n",
    "print(f\"\\nUsing Baseline Features ({len(baseline_features)}): {baseline_features}\")\n",
    "print(f\"Using Wildfire Features Only ({len(wildfire_features_only)}): {wildfire_features_only}\")\n",
    "print(f\"Using Interaction Features ({len(interaction_features)}): {interaction_features}\")\n",
    "print(f\"Using All Features for WF model ({len(all_features)}): {all_features}\")\n",
    "print(f\"\\nImputation Rules:\")\n",
    "print(f\" Zero-fill NaNs for ({len(zero_fill_fire_metrics)}): {zero_fill_fire_metrics}\")\n",
    "print(f\" Grouped median impute for ({len(grouped_median_impute_cols)}): {grouped_median_impute_cols}\")\n",
    "print(f\" Interaction handling for ({len(interaction_pairs)}): {list(interaction_pairs.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05ea9634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 3: Splitting Data: Train <= 2020, Test > 2020 ---\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'ZIP_CODE'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_5/g20j99hd5j91r849_d_zgb040000gn/T/ipykernel_93868/275309218.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# --- Step 3: Sort Data and Temporal Split ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\u001b[0m\u001b[0;34m\\n--- Step 3: Splitting Data: Train <= \u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mTRAIN_YEAR_CUTOFF\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m, Test > \u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mTRAIN_YEAR_CUTOFF\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m ---\u001b[0m\u001b[0;34m\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mYEAR_COLUMN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZIP_COLUMN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mYEAR_COLUMN\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mTRAIN_YEAR_CUTOFF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mYEAR_COLUMN\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mTRAIN_YEAR_CUTOFF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[1;32m   7168\u001b[0m                 \u001b[0;34mf\"\u001b[0m\u001b[0;34mLength of ascending (\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\"\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7169\u001b[0m                 \u001b[0;34mf\"\u001b[0m\u001b[0;34m != length of by (\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7170\u001b[0m             \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7172\u001b[0;31m             \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7174\u001b[0m             \u001b[0;31m# need to rewrap columns in Series to apply key function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7175\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1907\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1908\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1911\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1914\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ZIP_CODE'"
     ]
    }
   ],
   "source": [
    "# --- Step 3: Sort Data and Temporal Split ---\n",
    "print(f\"\\n--- Step 3: Splitting Data: Train <= {TRAIN_YEAR_CUTOFF}, Test > {TRAIN_YEAR_CUTOFF} ---\")\n",
    "data = data.sort_values(by=[YEAR_COLUMN, ZIP_COLUMN]).reset_index(drop=True)\n",
    "\n",
    "train_data = data[data[YEAR_COLUMN] <= TRAIN_YEAR_CUTOFF].copy()\n",
    "test_data = data[data[YEAR_COLUMN] > TRAIN_YEAR_CUTOFF].copy()\n",
    "\n",
    "if train_data.empty or test_data.empty:\n",
    "    raise ValueError(f\"Train or test split resulted in empty DataFrame. Check TRAIN_YEAR_CUTOFF ({TRAIN_YEAR_CUTOFF}).\")\n",
    "if not train_data.empty and not test_data.empty and train_data[YEAR_COLUMN].max() >= test_data[YEAR_COLUMN].min():\n",
    "     print(f\"Warning: Potential overlap/edge case in year split. Max train year: {train_data[YEAR_COLUMN].max()}, Min test year: {test_data[YEAR_COLUMN].min()}\")\n",
    "\n",
    "print(f\"Train data shape: {train_data.shape}\")\n",
    "print(f\"Test data shape:  {test_data.shape}\")\n",
    "\n",
    "y_train = train_data[TARGET_VARIABLE]\n",
    "y_test = test_data[TARGET_VARIABLE]\n",
    "\n",
    "# Get Raw Feature Sets for Preprocessing\n",
    "X_train_base_raw = train_data[baseline_features].copy()\n",
    "X_test_base_raw = test_data[baseline_features].copy()\n",
    "\n",
    "X_train_wf_raw = train_data[all_features].copy()\n",
    "X_test_wf_raw = test_data[all_features].copy()\n",
    "\n",
    "# --- Add FIRE_EXPOSED_COL temporarily if needed for grouped imputation ---\n",
    "# For baseline features\n",
    "if FIRE_EXPOSED_COL not in X_train_base_raw.columns and FIRE_EXPOSED_COL in train_data.columns:\n",
    "    X_train_base_raw[FIRE_EXPOSED_COL] = train_data[FIRE_EXPOSED_COL]\n",
    "    X_test_base_raw[FIRE_EXPOSED_COL] = test_data[FIRE_EXPOSED_COL]\n",
    "    print(f\"Temporarily added '{FIRE_EXPOSED_COL}' to baseline features for imputation.\")\n",
    "# For wildfire features (should already be there if defined correctly)\n",
    "if FIRE_EXPOSED_COL not in X_train_wf_raw.columns and FIRE_EXPOSED_COL in train_data.columns:\n",
    "     X_train_wf_raw[FIRE_EXPOSED_COL] = train_data[FIRE_EXPOSED_COL]\n",
    "     X_test_wf_raw[FIRE_EXPOSED_COL] = test_data[FIRE_EXPOSED_COL]\n",
    "     print(f\"Added '{FIRE_EXPOSED_COL}' to wildfire features (was missing).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7349a0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Preprocessing Pipeline (Grouped Impute THEN Scale) ---\n",
    "print(\"\\n--- Step 4: Preprocessing Pipeline (Grouped Impute THEN Scale) ---\")\n",
    "\n",
    "def preprocess_features_grouped_impute(X_train, X_test,\n",
    "                                      fire_exposed_col='FIRE_EXPOSED',\n",
    "                                      zero_fill_metrics=None,\n",
    "                                      grouped_median_cols=None,\n",
    "                                      interaction_pairs_dict=None):\n",
    "    \"\"\"\n",
    "    Applies grouped imputation based on FIRE_EXPOSED status, then scales.\n",
    "    Fits imputation stats and scaler ONLY on training data.\n",
    "    \"\"\"\n",
    "    print(f\"--- Preprocessing {len(X_train.columns)} features ---\")\n",
    "    X_train_proc = X_train.copy()\n",
    "    X_test_proc = X_test.copy()\n",
    "    feature_names = X_train.columns.tolist()\n",
    "    imputation_values_train = {}\n",
    "\n",
    "    if fire_exposed_col not in X_train.columns or fire_exposed_col not in X_test.columns:\n",
    "         raise ValueError(f\"'{fire_exposed_col}' column missing from input DataFrames for preprocessing.\")\n",
    "\n",
    "    # --- Imputation Strategy ---\n",
    "\n",
    "    # 1. Handle explicit zero-fill metrics first\n",
    "    if zero_fill_metrics:\n",
    "        actual_zero_fill = [col for col in zero_fill_metrics if col in feature_names]\n",
    "        if actual_zero_fill:\n",
    "            print(f\" 1. Zero-filling NaNs for: {actual_zero_fill}\")\n",
    "            for col in actual_zero_fill:\n",
    "                X_train_proc[col] = X_train_proc[col].fillna(0)\n",
    "                X_test_proc[col] = X_test_proc[col].fillna(0)\n",
    "        else:\n",
    "            print(\" 1. No specified zero-fill metrics found in columns.\")\n",
    "\n",
    "    # 2. Handle Interaction Terms\n",
    "    if interaction_pairs_dict:\n",
    "        print(\" 2. Handling interaction terms...\")\n",
    "        for interaction_term, components in interaction_pairs_dict.items():\n",
    "            if interaction_term not in feature_names: continue\n",
    "\n",
    "            fire_comp, _ = components # ('FIRE_RELATED_VAR', 'OTHER_VAR')\n",
    "\n",
    "            # Need fire component values (use 0 if NaN after potential zero-fill)\n",
    "            fire_component_values_train = X_train_proc[fire_comp].fillna(0)\n",
    "            fire_component_values_test = X_test_proc[fire_comp].fillna(0)\n",
    "\n",
    "            # Condition where interaction should be 0 (fire component is 0)\n",
    "            zero_condition_train = (fire_component_values_train == 0)\n",
    "            zero_condition_test = (fire_component_values_test == 0)\n",
    "\n",
    "            # Impute with 0 where condition is met AND interaction term is NaN\n",
    "            train_null_mask = X_train_proc[interaction_term].isnull()\n",
    "            test_null_mask = X_test_proc[interaction_term].isnull()\n",
    "            X_train_proc.loc[zero_condition_train & train_null_mask, interaction_term] = 0\n",
    "            X_test_proc.loc[zero_condition_test & test_null_mask, interaction_term] = 0\n",
    "\n",
    "            # For remaining NaNs (where fire comp > 0), use grouped median from TRAIN\n",
    "            if X_train_proc.loc[~zero_condition_train, interaction_term].isnull().any():\n",
    "                 median_interaction_fire = X_train_proc.loc[~zero_condition_train, interaction_term].median()\n",
    "                 imputation_values_train[interaction_term + '_fire'] = median_interaction_fire\n",
    "\n",
    "                 # Apply to remaining NaNs in both train and test\n",
    "                 X_train_proc.loc[~zero_condition_train & X_train_proc[interaction_term].isnull(), interaction_term] = median_interaction_fire\n",
    "                 X_test_proc.loc[~zero_condition_test & X_test_proc[interaction_term].isnull(), interaction_term] = median_interaction_fire\n",
    "            else:\n",
    "                 # Store NaN or 0 if no valid median could be calculated (e.g., all NaNs or no fire>0 cases)\n",
    "                 imputation_values_train[interaction_term + '_fire'] = X_train_proc.loc[~zero_condition_train, interaction_term].median() # Recalc to store NaN if needed\n",
    "                 # Apply this value (could be NaN) - will be handled by fallback imputer if necessary\n",
    "                 X_test_proc.loc[~zero_condition_test & X_test_proc[interaction_term].isnull(), interaction_term] = imputation_values_train[interaction_term + '_fire']\n",
    "\n",
    "\n",
    "    # 3. Grouped Median Imputation for other columns\n",
    "    if grouped_median_cols:\n",
    "        print(f\" 3. Applying grouped median imputation for: {grouped_median_cols}\")\n",
    "        # Get indices for fire/no-fire groups IN TRAINING DATA\n",
    "        train_fire_idx = X_train_proc[fire_exposed_col] == 1\n",
    "        train_no_fire_idx = X_train_proc[fire_exposed_col] == 0\n",
    "        # Get indices for TEST data groups\n",
    "        test_fire_idx = X_test_proc[fire_exposed_col] == 1\n",
    "        test_no_fire_idx = X_test_proc[fire_exposed_col] == 0\n",
    "\n",
    "        for col in grouped_median_cols:\n",
    "            if col not in feature_names: continue\n",
    "            if X_train_proc[col].isnull().any() or X_test_proc[col].isnull().any(): # Check if imputation needed\n",
    "                # Calculate medians ONLY from Training data groups\n",
    "                median_no_fire = X_train_proc.loc[train_no_fire_idx, col].median()\n",
    "                median_fire = X_train_proc.loc[train_fire_idx, col].median()\n",
    "\n",
    "                # Store medians\n",
    "                imputation_values_train[col + '_no_fire'] = median_no_fire\n",
    "                imputation_values_train[col + '_fire'] = median_fire\n",
    "\n",
    "                # Apply to TRAIN set\n",
    "                X_train_proc.loc[train_no_fire_idx, col] = X_train_proc.loc[train_no_fire_idx, col].fillna(median_no_fire)\n",
    "                X_train_proc.loc[train_fire_idx, col] = X_train_proc.loc[train_fire_idx, col].fillna(median_fire)\n",
    "\n",
    "                # Apply to TEST set (using medians learned from TRAIN)\n",
    "                X_test_proc.loc[test_no_fire_idx, col] = X_test_proc.loc[test_no_fire_idx, col].fillna(median_no_fire)\n",
    "                X_test_proc.loc[test_fire_idx, col] = X_test_proc.loc[test_fire_idx, col].fillna(median_fire)\n",
    "\n",
    "    # 4. Final check for any remaining NaNs (use global median as fallback)\n",
    "    fallback_imputer = SimpleImputer(strategy='median')\n",
    "    try:\n",
    "        # Check NaNs *after* all specific imputations\n",
    "        if X_train_proc.isnull().values.any():\n",
    "            nan_cols_train = X_train_proc.columns[X_train_proc.isnull().any()].tolist()\n",
    "            print(f\" 4. WARNING: NaNs still present after specific imputations in TRAIN columns: {nan_cols_train}. Applying global median fallback...\")\n",
    "            X_train_proc[nan_cols_train] = fallback_imputer.fit_transform(X_train_proc[nan_cols_train])\n",
    "            # Check and apply to test set for same columns if needed\n",
    "            nan_cols_test = X_test_proc.columns[X_test_proc.isnull().any()].tolist()\n",
    "            cols_to_transform_test = [col for col in nan_cols_train if col in nan_cols_test]\n",
    "            if cols_to_transform_test:\n",
    "                 print(f\"    Applying fallback imputation to TEST columns: {cols_to_transform_test}\")\n",
    "                 X_test_proc[cols_to_transform_test] = fallback_imputer.transform(X_test_proc[cols_to_transform_test])\n",
    "        else:\n",
    "             print(\" 4. No NaNs found after specific imputations. Skipping fallback.\")\n",
    "             # Fit the imputer anyway in case test set has NaNs in columns train didn't\n",
    "             fallback_imputer.fit(X_train_proc)\n",
    "\n",
    "\n",
    "        # Final check on TEST set for any NaNs missed (e.g., column had no NaNs in train but does in test)\n",
    "        if X_test_proc.isnull().values.any():\n",
    "             nan_cols_test_final = X_test_proc.columns[X_test_proc.isnull().any()].tolist()\n",
    "             print(f\"    WARNING: NaNs found in TEST columns: {nan_cols_test_final} possibly missed by train fit. Applying fallback transform...\")\n",
    "             # Use the imputer fitted on train data\n",
    "             X_test_proc[nan_cols_test_final] = fallback_imputer.transform(X_test_proc[nan_cols_test_final])\n",
    "\n",
    "\n",
    "    except ValueError as e:\n",
    "         print(f\"ERROR during fallback imputation: {e}. NaNs might remain.\")\n",
    "\n",
    "\n",
    "    # --- Scaling Step ---\n",
    "    print(\" 5. Applying StandardScaler...\")\n",
    "    scaler = StandardScaler()\n",
    "    # Ensure column order is the same before scaling\n",
    "    X_train_proc = X_train_proc[feature_names]\n",
    "    X_test_proc = X_test_proc[feature_names]\n",
    "\n",
    "    X_train_scaled = scaler.fit_transform(X_train_proc)\n",
    "    X_test_scaled = scaler.transform(X_test_proc)\n",
    "\n",
    "    # Convert back to DataFrame\n",
    "    X_train_processed = pd.DataFrame(X_train_scaled, columns=feature_names, index=X_train.index)\n",
    "    X_test_processed = pd.DataFrame(X_test_scaled, columns=feature_names, index=X_test.index)\n",
    "\n",
    "    print(\"--- Preprocessing Complete ---\")\n",
    "    return X_train_processed, X_test_processed, scaler, imputation_values_train # Return learned values\n",
    "\n",
    "\n",
    "# --- Apply Preprocessing ---\n",
    "\n",
    "print(\"\\nProcessing Baseline Features (Grouped Impute THEN Scale)...\")\n",
    "X_train_base_processed, X_test_base_processed, scaler_base, impute_vals_base = preprocess_features_grouped_impute(\n",
    "    X_train_base_raw, X_test_base_raw,\n",
    "    fire_exposed_col=FIRE_EXPOSED_COL,\n",
    "    zero_fill_metrics=[], # No specific fire metrics in baseline usually\n",
    "    grouped_median_cols= [c for c in grouped_median_impute_cols if c in baseline_features], # Only impute baseline cols\n",
    "    interaction_pairs_dict={} # No interactions in baseline\n",
    ")\n",
    "print(\"Baseline features processed.\")\n",
    "\n",
    "# Remove the temporarily added FIRE_EXPOSED_COL from baseline processed sets if it wasn't originally intended\n",
    "if FIRE_EXPOSED_COL in X_train_base_processed.columns and FIRE_EXPOSED_COL not in baseline_features:\n",
    "    X_train_base_processed = X_train_base_processed.drop(columns=[FIRE_EXPOSED_COL])\n",
    "    X_test_base_processed = X_test_base_processed.drop(columns=[FIRE_EXPOSED_COL])\n",
    "    print(f\"Removed temporary '{FIRE_EXPOSED_COL}' from processed baseline features.\")\n",
    "\n",
    "\n",
    "print(\"\\nProcessing All Features (Wildfire Included - Grouped Impute THEN Scale)...\")\n",
    "X_train_wf_processed, X_test_wf_processed, scaler_wf, impute_vals_wf = preprocess_features_grouped_impute(\n",
    "    X_train_wf_raw, X_test_wf_raw,\n",
    "    fire_exposed_col=FIRE_EXPOSED_COL,\n",
    "    zero_fill_metrics=zero_fill_fire_metrics,\n",
    "    grouped_median_cols=grouped_median_impute_cols,\n",
    "    interaction_pairs_dict=interaction_pairs\n",
    ")\n",
    "print(\"All features (including wildfire) processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c49d470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4.5: Train Benchmark Linear Regression Models ---\n",
    "print(\"\\n--- Step 4.5: Training Benchmark Linear Regression Models ---\")\n",
    "\n",
    "# --- Train Linear Regression (Baseline Features) ---\n",
    "print(\"Training Linear Regression (Baseline - Grouped Imputed)...\")\n",
    "lr_base = LinearRegression(n_jobs=-1) # Use n_jobs=-1 for potential parallelization if applicable\n",
    "# Ensure processed data is used and has no NaNs\n",
    "if X_train_base_processed.isnull().values.any():\n",
    "    raise ValueError(\"NaNs detected in X_train_base_processed before LR training!\")\n",
    "lr_base.fit(X_train_base_processed, y_train)\n",
    "print(\"Linear Regression (Baseline) trained.\")\n",
    "\n",
    "# --- Train Linear Regression (Wildfire Features) ---\n",
    "print(\"Training Linear Regression (Wildfire - Grouped Imputed)...\")\n",
    "lr_wf = LinearRegression(n_jobs=-1)\n",
    "# Ensure processed data is used and has no NaNs\n",
    "if X_train_wf_processed.isnull().values.any():\n",
    "    raise ValueError(\"NaNs detected in X_train_wf_processed before LR training!\")\n",
    "lr_wf.fit(X_train_wf_processed, y_train)\n",
    "print(\"Linear Regression (Wildfire) trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc63a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 5: Cross-Validation and Hyperparameter Tuning ---\n",
    "print(\"\\n--- Step 5: Cross-Validation and Hyperparameter Tuning ---\")\n",
    "\n",
    "# Define Temporal Cross-Validation Strategy\n",
    "tscv = TimeSeriesSplit(n_splits=N_CV_SPLITS)\n",
    "\n",
    "# --- Tuning Function ---\n",
    "def tune_xgboost(X_train, y_train, param_grid, cv_strategy):\n",
    "    \"\"\"Performs GridSearchCV for XGBoost.\"\"\"\n",
    "    xgb = XGBRegressor(objective='reg:squarederror', random_state=42, n_jobs=-1)\n",
    "    search = GridSearchCV(xgb, param_grid=param_grid,\n",
    "                          scoring='neg_root_mean_squared_error', # Optimize for lower RMSE\n",
    "                          n_jobs=-1, cv=cv_strategy, verbose=1)\n",
    "    try:\n",
    "        search.fit(X_train, y_train)\n",
    "        print(f\"Best parameters found: {search.best_params_}\")\n",
    "        print(f\"Best CV score (neg_RMSE): {search.best_score_:.4f}\")\n",
    "        return search.best_estimator_, search.best_params_\n",
    "    except ValueError as e:\n",
    "        print(f\"ERROR during GridSearchCV: {e}\")\n",
    "        print(\"Tuning failed, returning default XGBoost model.\")\n",
    "        # Fallback to default model if tuning fails\n",
    "        default_xgb = XGBRegressor(objective='reg:squarederror', random_state=42, n_jobs=-1)\n",
    "        default_xgb.fit(X_train, y_train)\n",
    "        return default_xgb, {} # Return default model and empty params\n",
    "\n",
    "# --- Tune Baseline Model ---\n",
    "print(\"\\nTuning XGBoost (Baseline - Grouped Imputed)...\")\n",
    "# Ensure processed data is used and has no NaNs\n",
    "if X_train_base_processed.isnull().values.any():\n",
    "    raise ValueError(\"NaNs detected in X_train_base_processed before tuning!\")\n",
    "best_xgb_base, best_params_base = tune_xgboost(X_train_base_processed, y_train, XGB_PARAM_GRID, tscv)\n",
    "\n",
    "# --- Tune Wildfire Model ---\n",
    "print(\"\\nTuning XGBoost (Wildfire - Grouped Imputed)...\")\n",
    "if X_train_wf_processed.isnull().values.any():\n",
    "    raise ValueError(\"NaNs detected in X_train_wf_processed before tuning!\")\n",
    "best_xgb_wf, best_params_wf = tune_xgboost(X_train_wf_processed, y_train, XGB_PARAM_GRID, tscv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d280f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 6: Evaluation on Test Set with Tuned and Benchmark Models ---\n",
    "print(\"\\n--- Step 6: Evaluating Tuned XGBoost and Benchmark LR Models on Test Set ---\")\n",
    "\n",
    "results = {}\n",
    "predictions = {}\n",
    "\n",
    "models_to_evaluate = {\n",
    "    # Benchmark Linear Regression Models\n",
    "    \"Linear Regression (Baseline GI)\": (lr_base, X_test_base_processed),\n",
    "    \"Linear Regression (Wildfire GI)\": (lr_wf, X_test_wf_processed),\n",
    "\n",
    "    # Tuned XGBoost Models\n",
    "    f\"XGBoost (Baseline - Tuned GI)\": (best_xgb_base, X_test_base_processed),\n",
    "    f\"XGBoost (Wildfire - Tuned GI)\": (best_xgb_wf, X_test_wf_processed)\n",
    "}\n",
    "\n",
    "for name, (model, X_test_data) in models_to_evaluate.items():\n",
    "    if X_test_data.isnull().values.any():\n",
    "         print(f\"CRITICAL WARNING: NaNs detected in FINAL test data for {name} before prediction. Check preprocessing logic.\")\n",
    "\n",
    "    y_pred = model.predict(X_test_data)\n",
    "    predictions[name] = y_pred\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    results[name] = {\"RMSE\": rmse, \"MAE\": mae, \"R2\": r2}\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE:  {mae:.4f}\")\n",
    "    print(f\"R2:   {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156a8cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 7: Compare Results Summary ---\n",
    "print(\"\\n--- Step 7: Comparison Summary (Tuned Models, Grouped Imputed - Test Set Performance) ---\")\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(results_df.sort_values(by='R2', ascending=False))\n",
    "\n",
    "# --- R2 Improvement Check ---\n",
    "print(\"\\n--- R2 Improvement from Wildfire Features (Tuned Models, Grouped Imputed) ---\")\n",
    "base_name = f\"XGBoost (Baseline - Tuned GI)\"\n",
    "wf_name = f\"XGBoost (Wildfire - Tuned GI)\"\n",
    "if base_name in results and wf_name in results:\n",
    "    base_r2 = results[base_name][\"R2\"]\n",
    "    wf_r2 = results[wf_name][\"R2\"]\n",
    "    improvement = wf_r2 - base_r2\n",
    "    print(f\"Baseline R2: {base_r2:.4f}\")\n",
    "    print(f\"Wildfire R2: {wf_r2:.4f}\")\n",
    "    print(f\"Improvement: {improvement:.4f} ({'IMPROVED' if improvement > 0 else ('WORSENED' if improvement < 0 else 'NO CHANGE')})\")\n",
    "else:\n",
    "    print(\"Could not compare tuned XGBoost models (Grouped Imputed), results missing.\")\n",
    "\n",
    "print(\"\\n--- R2 Improvement from Wildfire Features (Linear Regression Models, Grouped Imputed) ---\")\n",
    "base_name_lr = \"Linear Regression (Baseline GI)\"\n",
    "wf_name_lr = \"Linear Regression (Wildfire GI)\"\n",
    "if base_name_lr in results and wf_name_lr in results:\n",
    "    base_r2_lr = results[base_name_lr][\"R2\"]\n",
    "    wf_r2_lr = results[wf_name_lr][\"R2\"]\n",
    "    improvement_lr = wf_r2_lr - base_r2_lr\n",
    "    print(f\"(LR) Baseline R2: {base_r2_lr:.4f}\")\n",
    "    print(f\"(LR) Wildfire R2: {wf_r2_lr:.4f}\")\n",
    "    print(f\"(LR) Improvement: {improvement_lr:.4f} ({'IMPROVED' if improvement_lr > 0 else ('WORSENED' if improvement_lr < 0 else 'NO CHANGE')})\")\n",
    "else:\n",
    "    print(\"Could not compare Linear Regression models, results missing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd22ec19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 8: Rigorous Extraction of Wildfire Effect using SHAP ---\n",
    "print(\"\\n--- Step 8: Extracting Wildfire Effect using SHAP ---\")\n",
    "\n",
    "model_for_shap = best_xgb_wf\n",
    "# Explain using the *final processed* training data (Grouped Imputed then Scaled)\n",
    "X_shap_train = X_train_wf_processed\n",
    "X_shap_test = X_test_wf_processed # Explain test predictions using processed test data\n",
    "\n",
    "print(\"Initializing SHAP TreeExplainer...\")\n",
    "explainer = shap.TreeExplainer(model_for_shap)\n",
    "\n",
    "# Calculate SHAP values\n",
    "print(\"Calculating SHAP values for the test set (might take time)...\")\n",
    "# Check for NaNs before SHAP calculation\n",
    "if X_shap_test.isnull().values.any():\n",
    "        raise ValueError(\"NaNs found in data provided to SHAP explainer. Check preprocessing.\")\n",
    "shap_values_test = explainer.shap_values(X_shap_test)\n",
    "\n",
    "\n",
    "# --- SHAP Plots using Test Set Explanations ---\n",
    "\n",
    "print(\"\\n1. SHAP Global Feature Importance (Test Set):\")\n",
    "plt.figure(figsize=(10, max(8, len(X_shap_test.columns) * 0.3)))\n",
    "shap.summary_plot(shap_values_test, X_shap_test, plot_type=\"bar\", show=False)\n",
    "plt.title(\"SHAP Global Importance (Test Set)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n2. SHAP Summary Plot (Beeswarm - Test Set):\")\n",
    "plt.figure(figsize=(10, max(8, len(X_shap_test.columns) * 0.3)))\n",
    "shap.summary_plot(shap_values_test, X_shap_test, show=False)\n",
    "plt.title(\"SHAP Summary Plot (Test Set)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n3. SHAP Dependence Plots for Key Wildfire Features (Test Set):\")\n",
    "# Select key features relevant to wildfire impact\n",
    "key_wf_features_for_plot = [\n",
    "    'MAX_PCT_ZIP_BURNED', 'TOTAL_ACRES_BURNED_IN_ZIP', FIRE_EXPOSED_COL,\n",
    "    'YEARS_SINCE_LAST_FIRE', 'FIRE_SHOCK',\n",
    "    # Include important interaction terms if they exist\n",
    "    'MAX_PCT_ZIP_BURNED_x_ROLLING_1yr_PRICE_CHANGE', 'FIRE_EXPOSED_x_Median_Household_Income'\n",
    "    ]\n",
    "key_wf_features_for_plot = [f for f in key_wf_features_for_plot if f in X_shap_test.columns] # Ensure they exist\n",
    "\n",
    "for feature in key_wf_features_for_plot:\n",
    "    try:\n",
    "        plt.figure()\n",
    "        shap.dependence_plot(feature, shap_values_test, X_shap_test, interaction_index=None, show=False)\n",
    "        plt.title(f\"SHAP Dependence Plot: {feature} (Test Set)\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "            print(f\"Could not generate dependence plot for {feature}: {e}\")\n",
    "\n",
    "# 4. Compare Predictions Directly\n",
    "print(\"\\n4. Comparing Predictions: Wildfire Model vs Baseline Model (Test Set)\")\n",
    "pred_base = predictions.get(f\"XGBoost (Baseline - Tuned GI)\")\n",
    "pred_wf = predictions.get(f\"XGBoost (Wildfire - Tuned GI)\")\n",
    "\n",
    "if pred_base is not None and pred_wf is not None:\n",
    "    prediction_diff = pred_wf - pred_base\n",
    "    # Add predictions back to the original test_data for context\n",
    "    test_data_with_preds = test_data.copy()\n",
    "    test_data_with_preds['pred_base_gi'] = pred_base\n",
    "    test_data_with_preds['pred_wf_gi'] = pred_wf\n",
    "    test_data_with_preds['prediction_diff_gi'] = prediction_diff\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(prediction_diff, kde=True)\n",
    "    plt.title(\"Distribution of Prediction Difference (Wildfire Model - Baseline Model)\")\n",
    "    plt.xlabel(\"Prediction Difference (WF Model - Baseline Model)\")\n",
    "    plt.show()\n",
    "\n",
    "    # Analyze difference based on fire presence using the original FIRE_EXPOSED_COL\n",
    "    if FIRE_EXPOSED_COL in test_data_with_preds.columns:\n",
    "        diff_summary = test_data_with_preds.groupby(FIRE_EXPOSED_COL)['prediction_diff_gi'].agg(['mean', 'median', 'std', 'count'])\n",
    "        print(f\"\\nPrediction Difference Summary by '{FIRE_EXPOSED_COL}':\")\n",
    "        print(diff_summary)\n",
    "\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        sns.boxplot(data=test_data_with_preds, x=FIRE_EXPOSED_COL, y='prediction_diff_gi')\n",
    "        plt.title(f\"Prediction Difference vs {FIRE_EXPOSED_COL}\")\n",
    "        plt.ylabel(\"Prediction Difference (WF Model - Baseline Model)\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Column '{FIRE_EXPOSED_COL}' not found in test_data_with_preds for difference analysis.\")\n",
    "else:\n",
    "    print(\"Could not compare predictions directly as one or both model predictions are missing.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
