{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01ac39a7",
   "metadata": {},
   "source": [
    "# Model on PCT_PRICE_CHANGE_DETRENDED (%) <= 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ed3d7c",
   "metadata": {},
   "source": [
    "## Step 0: Preparation: Load the processed content and original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "523e1300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Detrending Target Variable ---\n",
      "Target variable 'PCT_PRICE_CHANGE_DETRENDED (%)' created.\n",
      "Target variable mean: 0.0000, std: 10.5283\n",
      "\n",
      "--- Defining Feature Sets ---\n",
      "\n",
      "--- Splitting Data: Train <= 2020, Test > 2020 ---\n",
      "\n",
      "--- Applying Imputation and Scaling ---\n",
      "Imputed NaNs in All features using median strategy.\n",
      "Scaled All features using StandardScaler.\n",
      "Imputation and Scaling complete.\n",
      "\n",
      "--- Training Models ---\n",
      "Trained Random Forest (Wildfire Features).\n",
      "Trained XGBoost (Wildfire Features).\n",
      "get the original model prediciton\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "data = pd.read_csv(\"../10_Data_Clean/model_track_data.csv\")\n",
    "\n",
    "# Configuration\n",
    "TARGET_VARIABLE = 'PCT_PRICE_CHANGE_DETRENDED (%)'\n",
    "RAW_TARGET_VARIABLE = 'PCT_PRICE_CHANGE (%)'\n",
    "YEAR_COLUMN = 'YEAR'\n",
    "GEO_UNIQUE_ID_COLUMN = 'GEO_UNIQUE_ID'\n",
    "TRAIN_YEAR_CUTOFF = 2020\n",
    "\n",
    "# XGBoost specific params (can be tuned later)\n",
    "XGB_PARAMS = {\n",
    "    'objective': 'reg:squarederror', # Objective function for regression\n",
    "    'n_estimators': 100,            # Number of boosting rounds/trees\n",
    "    'learning_rate': 0.1,           # Step size shrinkage\n",
    "    'max_depth': 5,                 # Maximum tree depth\n",
    "    'subsample': 0.8,               # Fraction of samples used per tree\n",
    "    'colsample_bytree': 0.8,        # Fraction of features used per tree\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1                    # Use all available CPU cores\n",
    "}\n",
    "\n",
    "# Step 1: Detrending Target Variable\n",
    "print(\"--- Detrending Target Variable ---\")\n",
    "# Ensure YEAR is numeric\n",
    "data[YEAR_COLUMN] = pd.to_numeric(data[YEAR_COLUMN], errors='coerce')\n",
    "data = data.dropna(subset=[YEAR_COLUMN, RAW_TARGET_VARIABLE]) # Drop rows where year or target is missing\n",
    "\n",
    "trend_model = LinearRegression()\n",
    "# Reshape YEAR for sklearn compatibility\n",
    "trend_model.fit(data[[YEAR_COLUMN]], data[RAW_TARGET_VARIABLE])\n",
    "predicted_trend = trend_model.predict(data[[YEAR_COLUMN]])\n",
    "data[TARGET_VARIABLE] = data[RAW_TARGET_VARIABLE] - predicted_trend\n",
    "print(f\"Target variable '{TARGET_VARIABLE}' created.\")\n",
    "print(f\"Target variable mean: {data[TARGET_VARIABLE].mean():.4f}, std: {data[TARGET_VARIABLE].std():.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Define Feature Sets\n",
    "print(\"\\n--- Defining Feature Sets ---\")\n",
    "# Carefully select features, avoiding leakage from future/target variables\n",
    "baseline_features = [\n",
    "    'Median_Household_Income', 'Total_Population', 'Avg_Household_Size',\n",
    "    'Gini_Index', 'Employment_Rate', 'Below_Poverty_Rate',\n",
    "    'Rate_College_or_Higher', 'Black_Portion', 'White_Portion',\n",
    "    'American_Indian_and_Alaska_Native_Portion', 'Asian_Portion',\n",
    "    # Lagged Price Features\n",
    "    'HOME_PRICE_LAG1', 'PRICE_CHANGE_LAG1', 'PRICE_CHANGE_DIFF',\n",
    "    'ROLLING_1yr_PRICE_CHANGE', 'ROLLING_2yr_PRICE_CHANGE_STD'\n",
    "    # Avoid interaction terms involving FIRE here\n",
    "]\n",
    "\n",
    "wildfire_features = [\n",
    "    'NUM_FIRES', 'TOTAL_AREA_BURNED_IN_M2', 'AVG_FIRE_DURATION_DAYS',\n",
    "    'MAX_PCT_TRACT_BURNED', 'ANY_MAJOR_FIRE', 'FIRE_EXPOSED',\n",
    "    'PREV_MAX_PCT_TRACT_BURNED', 'FIRE_LAST_YEAR', 'FIRE_SHOCK',\n",
    "    'YEARS_SINCE_LAST_FIRE', 'CUMULATIVE_AREA_BURNED_LAST_2YRS',\n",
    "    # Interaction terms\n",
    "    'FIRE_EXPOSED_x_PRICE_CHANGE_LAG1',\n",
    "    'MAX_PCT_TRACT_BURNED_x_ROLLING_1yr_PRICE_CHANGE',\n",
    "    'FIRE_EXPOSED_x_Median_Household_Income',\n",
    "    'FIRE_EXPOSED_x_Below_Poverty_Rate'\n",
    "]\n",
    "\n",
    "# Ensure all defined features exist in the dataframe columns\n",
    "baseline_features = [f for f in baseline_features if f in data.columns]\n",
    "wildfire_features = [f for f in wildfire_features if f in data.columns]\n",
    "\n",
    "# Combine for the second model\n",
    "temp_all_features = list(set(baseline_features + wildfire_features)) \n",
    "all_features = sorted(temp_all_features)\n",
    "\n",
    "# Verify no target/future leakage in feature lists\n",
    "leaky_features = ['PRICE', 'NEXT_YEAR_PRICE', 'PRICE_CHANGE', RAW_TARGET_VARIABLE, TARGET_VARIABLE]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Sort Data and Temporal Split\n",
    "print(f\"\\n--- Splitting Data: Train <= {TRAIN_YEAR_CUTOFF}, Test > {TRAIN_YEAR_CUTOFF} ---\")\n",
    "data = data.sort_values(by=[YEAR_COLUMN, GEO_UNIQUE_ID_COLUMN]).reset_index(drop=True)\n",
    "\n",
    "train_data = data[data[YEAR_COLUMN] <= TRAIN_YEAR_CUTOFF].copy()\n",
    "test_data = data[data[YEAR_COLUMN] > TRAIN_YEAR_CUTOFF].copy()\n",
    "\n",
    "\n",
    "# Separate features (X) and target (y) for both sets\n",
    "y_train = train_data[TARGET_VARIABLE]\n",
    "y_test = test_data[TARGET_VARIABLE]\n",
    "\n",
    "X_train_base = train_data[baseline_features]\n",
    "X_test_base = test_data[baseline_features]\n",
    "\n",
    "X_train_wf = train_data[all_features]\n",
    "X_test_wf = test_data[all_features]\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Step 4: Imputation and Scaling\n",
    "print(\"\\n--- Applying Imputation and Scaling ---\")\n",
    "\n",
    "# --- Imputation ---\n",
    "# Fit Imputer ONLY on training data, then transform both train and test\n",
    "imputer_wf = SimpleImputer(strategy='median')\n",
    "X_train_wf_imputed = imputer_wf.fit_transform(X_train_wf)\n",
    "X_test_wf_imputed = imputer_wf.transform(X_test_wf)\n",
    "print(f\"Imputed NaNs in All features using median strategy.\")\n",
    "\n",
    "# --- Scaling ---\n",
    "scaler_wf = StandardScaler()\n",
    "X_train_wf_scaled = scaler_wf.fit_transform(X_train_wf_imputed)\n",
    "X_test_wf_scaled = scaler_wf.transform(X_test_wf_imputed)\n",
    "print(f\"Scaled All features using StandardScaler.\")\n",
    "\n",
    "# Convert back to DataFrames (maintains column names for importance plots)\n",
    "X_train_wf_scaled = pd.DataFrame(X_train_wf_scaled, columns=all_features, index=X_train_wf.index)\n",
    "X_test_wf_scaled = pd.DataFrame(X_test_wf_scaled, columns=all_features, index=X_test_wf.index)\n",
    "\n",
    "print(\"Imputation and Scaling complete.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 5: Model Training\n",
    "print(\"\\n--- Training Models ---\")\n",
    "\n",
    "\n",
    "rf_wf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1, max_depth=10, min_samples_leaf=5)\n",
    "rf_wf.fit(X_train_wf_scaled, y_train)\n",
    "print(\"Trained Random Forest (Wildfire Features).\")\n",
    "\n",
    "# --- Original XGBoost ---\n",
    "xgb_wf = XGBRegressor(**XGB_PARAMS)\n",
    "xgb_wf.fit(X_train_wf_scaled, y_train)\n",
    "print(\"Trained XGBoost (Wildfire Features).\")\n",
    "\n",
    "\n",
    "y_original_pred_xgb_wf = xgb_wf.predict(X_test_wf_scaled)\n",
    "print(\"get the original model prediciton\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c47112",
   "metadata": {},
   "source": [
    "## Step 1: Filter out samples with y<=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "624abe49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop Training Samples: 34639\n",
      "Drop Testing Samples:  7085\n"
     ]
    }
   ],
   "source": [
    "# Define threshold\n",
    "severe_threshold = 0\n",
    "\n",
    "# Train set: only drop samples\n",
    "drop_train_idx = (y_train < severe_threshold)\n",
    "X_train_drop = X_train_wf_scaled[drop_train_idx]\n",
    "y_train_drop = y_train[drop_train_idx]\n",
    "\n",
    "# Test set: only drop samples\n",
    "drop_test_idx = (y_test < severe_threshold)\n",
    "X_test_drop = X_test_wf_scaled[drop_test_idx]\n",
    "y_test_drop = y_test[drop_test_idx]\n",
    "\n",
    "print(f\"Drop Training Samples: {X_train_drop.shape[0]}\")\n",
    "print(f\"Drop Testing Samples:  {X_test_drop.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b2bdb0",
   "metadata": {},
   "source": [
    "## Step 2: Build a new regression model (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d028047d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Drop-Specific XGBoost Model.\n",
      "Predicted Drop-Specific XGBoost Model.\n",
      "\n",
      "=== Drop Model (XGBoost) ===\n",
      "RMSE: 3.0793\n",
      "R2:   0.6422\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Define XGBoost model\n",
    "drop_model_xgb = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# # Define LightGBM model\n",
    "# drop_model_lgbm = LGBMRegressor(\n",
    "#     objective='regression',\n",
    "#     n_estimators=100,\n",
    "#     learning_rate=0.1,\n",
    "#     max_depth=5,\n",
    "#     subsample=0.8,\n",
    "#     colsample_bytree=0.8,\n",
    "#     random_state=42,\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "\n",
    "# Train on y<0 subset\n",
    "drop_model_xgb.fit(X_train_drop, y_train_drop)\n",
    "# drop_model_lgbm.fit(X_train_drop, y_train_drop)\n",
    "\n",
    "print(\"Trained Drop-Specific XGBoost Model.\")\n",
    "# print(\"Trained Drop-Specific LightGBM Model.\")\n",
    "\n",
    "y_pred_drop_xgb = drop_model_xgb.predict(X_test_drop)\n",
    "print(\"Predicted Drop-Specific XGBoost Model.\")\n",
    "# Evaluate XGBoost\n",
    "rmse_drop_xgb = np.sqrt(mean_squared_error(y_test_drop, y_pred_drop_xgb))\n",
    "r2_drop_xgb = r2_score(y_test_drop, y_pred_drop_xgb)\n",
    "\n",
    "print(\"\\n=== Drop Model (XGBoost) ===\")\n",
    "print(f\"RMSE: {rmse_drop_xgb:.4f}\")\n",
    "print(f\"R2:   {r2_drop_xgb:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e209b77d",
   "metadata": {},
   "source": [
    "## Step 3 (main step): Improve model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9101a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n",
      "Best parameters found:\n",
      "{'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}\n",
      "\n",
      "=== Best Tuned XGBoost Drop Model ===\n",
      "RMSE: 2.6688\n",
      "R2:   0.7313\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "xgb_drop_base = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.05],      # 尝试正常和较小的学习率\n",
    "    'max_depth': [5, 6, 7],             # 树的最大深度\n",
    "    'n_estimators': [100, 200],         # 树的数量（配合学习率）\n",
    "    'subsample': [0.8, 1.0],            # 采样比例\n",
    "    'colsample_bytree': [0.8, 1.0]      # 特征采样比例\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_drop_base,\n",
    "    param_grid=param_grid,\n",
    "    scoring='r2',     \n",
    "    cv=3,               \n",
    "    verbose=1,\n",
    "    n_jobs=-1          \n",
    ")\n",
    "\n",
    "# Fit GridSearchCV on the drop dataset\n",
    "grid_search.fit(X_train_drop, y_train_drop)\n",
    "\n",
    "# Best model\n",
    "best_xgb_drop = grid_search.best_estimator_\n",
    "\n",
    "# Print best params\n",
    "print(\"Best parameters found:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_best_xgb = best_xgb_drop.predict(X_test_drop)\n",
    "\n",
    "rmse_best = np.sqrt(mean_squared_error(y_test_drop, y_pred_best_xgb))\n",
    "r2_best = r2_score(y_test_drop, y_pred_best_xgb)\n",
    "\n",
    "print(\"\\n=== Best Tuned XGBoost Drop Model ===\")\n",
    "print(f\"RMSE: {rmse_best:.4f}\")\n",
    "print(f\"R2:   {r2_best:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "449d1149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Updated XGBoost Drop Model (drop_model_xgb_2).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define new XGBoost Drop Model with updated parameters\n",
    "drop_model_xgb_2 = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=7,\n",
    "    subsample=1.0,\n",
    "    colsample_bytree=1.0,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train the model on the drop dataset\n",
    "drop_model_xgb_2.fit(X_train_drop, y_train_drop)\n",
    "\n",
    "print(\"Trained Updated XGBoost Drop Model (drop_model_xgb_2).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8f19c1",
   "metadata": {},
   "source": [
    "## Step 4: Predict on the test subset with y<0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ae2afce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Drop Model (XGBoost) ===\n",
      "RMSE: 2.7022\n",
      "R2:   0.7245\n"
     ]
    }
   ],
   "source": [
    "y_pred_drop_xgb_2 = drop_model_xgb_2.predict(X_test_drop)\n",
    "\n",
    "# Evaluate XGBoost\n",
    "rmse_drop_xgb_2 = np.sqrt(mean_squared_error(y_test_drop, y_pred_drop_xgb_2))\n",
    "r2_drop_xgb_2 = r2_score(y_test_drop, y_pred_drop_xgb_2)\n",
    "\n",
    "print(\"\\n=== Drop Model (XGBoost) ===\")\n",
    "print(f\"RMSE: {rmse_drop_xgb_2:.4f}\")\n",
    "print(f\"R2:   {r2_drop_xgb_2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7778bc",
   "metadata": {},
   "source": [
    "## Step 5: Print the final result comparison summary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3f11670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Summary Comparison ===\n",
      "Original Full Model:    RMSE = 3.6740, R2 = 0.4907\n",
      "Drop Model (XGBoost 1):    RMSE = 3.0793, R2 = 0.6422\n",
      "Drop Model (new XGBoost 2):    RMSE = 2.7022, R2 = 0.7245\n"
     ]
    }
   ],
   "source": [
    "# Predict using Original full model\n",
    "y_pred_original_drop = xgb_wf.predict(X_test_drop)\n",
    "\n",
    "# Evaluate original model\n",
    "rmse_original_drop = np.sqrt(mean_squared_error(y_test_drop, y_pred_original_drop))\n",
    "r2_original_drop = r2_score(y_test_drop, y_pred_original_drop)\n",
    "\n",
    "\n",
    "print(\"\\n=== Summary Comparison ===\")\n",
    "print(f\"Original Full Model:    RMSE = {rmse_original_drop:.4f}, R2 = {r2_original_drop:.4f}\")\n",
    "print(f\"Drop Model (XGBoost 1):    RMSE = {rmse_drop_xgb:.4f}, R2 = {r2_drop_xgb:.4f}\")\n",
    "print(f\"Drop Model (new XGBoost 2):    RMSE = {rmse_drop_xgb_2:.4f}, R2 = {r2_drop_xgb_2:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
